#!/usr/bin/env python3
"""
Docker Entrypoint –¥–ª—è PostgreSQL OrderBook Collector
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å 200 —Å–∏–º–≤–æ–ª–∞–º–∏ MM —Ñ–æ–∫—É—Å–∞
"""

import asyncio
import os
import sys
import logging
import signal
import time
from pathlib import Path
from datetime import timedelta
import aiohttp
from typing import cast

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ PYTHONPATH
sys.path.insert(0, '/app')

from collector.config.symbols_mm_focused import SYMBOLS_200, validate_symbols
from collector.ingestion.batch_ingestor import BatchIngestor
from collector.monitoring.health_monitor import MonitoringSystem
from collector.database.connection import DatabaseConnection

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/app/logs/collector.log')
    ]
)
logger = logging.getLogger(__name__)

class ProductionCollector:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è production —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–∂–µ—Å—Ç–æ—Ä–æ–≤: –æ—Å–Ω–æ–≤–Ω–æ–π (bt/tr) –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–ª—è depth
        self.ingestors = []
        self.monitoring_system = None
        self.db_connection = None
        self.shutdown_event = asyncio.Event()
        self.active_symbols = []
        
        # Environment variables
        self.database_url = os.getenv('DATABASE_URL')
        self.batch_size = int(os.getenv('BATCH_SIZE', '500'))
        self.flush_interval = int(os.getenv('FLUSH_INTERVAL', '30'))
        self.shards = int(os.getenv('SHARDS', '5'))
        self.monitoring_port = int(os.getenv('MONITORING_PORT', '8000'))
        self.binance_base_url = os.getenv('BINANCE_BASE_URL', 'https://fapi.binance.com').strip()
        self.binance_ws_url = os.getenv('BINANCE_WS_URL', 'wss://fstream.binance.com/ws/').strip()
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ç–æ—á–∫–æ–π
        self.total_symbols_limit = None
        try:
            _lim = os.getenv('TOTAL_SYMBOLS', '').strip()
            if _lim:
                self.total_symbols_limit = max(0, int(_lim))
        except Exception:
            self.total_symbols_limit = None
        self.starting_symbol = os.getenv('STARTING_SYMBOL', 'SOLUSDT').strip().upper()
        # Depth –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.enable_depth = os.getenv('ENABLE_DEPTH', 'false').strip().lower() in ('1', 'true', 'yes')
        self.depth_top_symbols_env = os.getenv('DEPTH_TOP_SYMBOLS', '')
        # –î–æ–ø. –∫–∞–Ω–∞–ª—ã markPrice/forceOrder (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ –≤–∫–ª—é—á–∞–µ–º –≤ —ç—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ, –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–æ—Ä–∫–µ—Ä–æ–º)
        self.enable_mark_price = os.getenv('ENABLE_MARK_PRICE', 'false').strip().lower() in ('1','true','yes')
        self.enable_force_order = os.getenv('ENABLE_FORCE_ORDER', 'false').strip().lower() in ('1','true','yes')
        # Watchdog –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.enable_db_watchdog = os.getenv('ENABLE_DB_WATCHDOG', 'true').strip().lower() in ('1','true','yes')
        try:
            self.db_watchdog_interval = int(os.getenv('DB_WATCHDOG_INTERVAL', '60'))  # —Å–µ–∫
            self.db_watchdog_threshold = int(os.getenv('DB_WATCHDOG_THRESHOLD', '120'))  # —Å–µ–∫
        except Exception:
            self.db_watchdog_interval = 60
            self.db_watchdog_threshold = 120
        
        if not self.database_url:
            raise ValueError("DATABASE_URL environment variable is required")
        # –î–ª—è —Ç–∞–π–ø—á–µ–∫–µ—Ä–∞ –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ str
        self.database_url = cast(str, self.database_url)
    
    async def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã"""
        logger.info("üîß Initializing database connection...")
        db_url: str = str(self.database_url)
        self.db_connection = DatabaseConnection(db_url)
        await self.db_connection.connect()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        schema_file = Path('/app/collector/database/schema.sql')
        if schema_file.exists():
            logger.info("üìã Creating database schema...")
            try:
                with open(schema_file, 'r') as f:
                    schema_sql = f.read()
                # –ü–æ–Ω–∏–∂–∞–µ–º lock_timeout –Ω–∞ —Å–µ—Å—Å–∏—é, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–∞—Ö
                try:
                    await self.db_connection.execute_script("SET lock_timeout TO '5s';")
                except Exception:
                    pass
                await self.db_connection.execute_script(schema_sql)
                logger.info("‚úÖ Database schema created successfully")
            except Exception as e:
                # –ù–µ —Ñ–µ–π–ª–∏–º –≤–µ—Å—å –∑–∞–ø—É—Å–∫: —Å—Ö–µ–º—ã —É–∂–µ –µ—Å—Ç—å, –æ—à–∏–±–æ–∫ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞
                logger.warning(f"‚ö†Ô∏è Schema creation skipped due to error: {e}")

            # –î–æ–ø. –≥–∞—Ä–∞–Ω—Ç–∏—è: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ON CONFLICT –Ω–∞ depth_events
            # –í –ø—Ä–æ–¥–µ –º–æ–≥ –±—ã—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç —Ä–∞–Ω–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ PK/unique ‚Äî —Å–æ–∑–¥–∞—ë–º idempotent-–∏–Ω–¥–µ–∫—Å
            try:
                ensure_idx_sql = (
                    """
                    CREATE UNIQUE INDEX IF NOT EXISTS uq_depth_events_symbol_time_final
                    ON marketdata.depth_events (symbol_id, ts_exchange, final_update_id);
                    """
                )
                await self.db_connection.execute_script(ensure_idx_sql)
                logger.info("‚úÖ Ensured unique index on marketdata.depth_events (symbol_id, ts_exchange, final_update_id)")
            except Exception as e:
                logger.error(f"‚ùå Failed to ensure unique index for depth_events: {e}")
        else:
            logger.warning("‚ö†Ô∏è Schema file not found, skipping schema creation")
    
    async def validate_symbols_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        logger.info("üéØ Validating Market Maker symbols configuration...")
        
        try:
            validate_symbols()
            logger.info(f"‚úÖ Validated {len(SYMBOLS_200)} symbols for MM analysis")
            logger.info(f"üìä Starting with: {SYMBOLS_200[0]}")
            logger.info(f"üìä Ultra low-cap symbols: {len(SYMBOLS_200[-30:])}")

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º Binance Futures USDT-–ø–µ—Ä–ø–∞–º
            # –°–Ω–∞—á–∞–ª–∞ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –Ω–∞ Binance Futures
            resolved = await self._resolve_futures_symbols(SYMBOLS_200)
            logger.info(f"‚úÖ Resolved {len(resolved)} valid Futures symbols out of {len(SYMBOLS_200)}")
            # –ü–æ—Ä—è–¥–æ–∫: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ SYMBOLS_200 (—É–±—ã–≤–∞–Ω–∏–µ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏),
            # –Ω–æ —Ä–æ—Ç–∏—Ä—É–µ–º —Ç–∞–∫, —á—Ç–æ–±—ã STARTING_SYMBOL –±—ã–ª –ø–µ—Ä–≤—ã–º, –∞ –¥–∞–ª–µ–µ ‚Äî –º–µ–Ω–µ–µ –ª–∏–∫–≤–∏–¥–Ω—ã–µ
            base_order = [s for s in SYMBOLS_200 if s in set(resolved)]
            if self.starting_symbol in base_order:
                idx = base_order.index(self.starting_symbol)
                ordered = base_order[idx:] + base_order[:idx]
            else:
                ordered = base_order
            # –õ–∏–º–∏—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–º–≤–æ–ª–æ–≤ TOTAL_SYMBOLS (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
            if self.total_symbols_limit and self.total_symbols_limit > 0:
                self.active_symbols = ordered[: self.total_symbols_limit]
            else:
                self.active_symbols = ordered
            logger.info(
                f"üìä Active symbols configured: {len(self.active_symbols)} (start='{self.starting_symbol}', limit={self.total_symbols_limit})"
            )
            if len(self.active_symbols) < len(SYMBOLS_200):
                missing = len(SYMBOLS_200) - len(self.active_symbols)
                logger.warning(f"‚ö†Ô∏è Filtered out {missing} symbols not present on Binance Futures USDT-perp")
        except Exception as e:
            logger.error(f"‚ùå Symbol validation failed: {e}")
            raise

    async def _resolve_futures_symbols(self, candidates):
        """–ó–∞–ø—Ä–æ—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö USDT-–ø–µ—Ä–ø–µ—Ç—É–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ Binance Futures –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."""
        base = self.binance_base_url.rstrip('/')
        url = f"{base}/fapi/v1/exchangeInfo"
        timeout = aiohttp.ClientTimeout(total=10)
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    symbols = data.get('symbols', [])
                    allowed = set(
                        s.get('symbol') for s in symbols
                        if s.get('contractType') in ('PERPETUAL', 'CURRENT_QUARTER', 'NEXT_QUARTER')
                        and s.get('status') == 'TRADING'
                        and s.get('quoteAsset') == 'USDT'
                    )
                    filtered = [sym for sym in candidates if sym in allowed]
                    return filtered
        except Exception as e:
            logger.error(f"‚ùå Failed to resolve futures symbols from {url}: {e}. Fallback to original list.")
            return list(candidates)
    
    async def start_batch_ingestores(self):
        """–ó–∞–ø—É—Å–∫ batch –∏–Ω–∂–µ—Å—Ç–æ—Ä–æ–≤: –æ—Å–Ω–æ–≤–Ω–æ–π (bt/tr) + depth@100ms –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

        –ü–æ–ª–∏—Ç–∏–∫–∞:
        - –ï—Å–ª–∏ ENABLE_DEPTH=false ‚Üí depth –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ–º.
        - –ï—Å–ª–∏ DEPTH_TOP_SYMBOLS –Ω–µ–ø—É—Å—Ç–æ–π ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ —è–≤–Ω—ã–π override (—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ active_symbols), –∏–Ω–∞—á–µ –±–µ—Ä—ë–º –≤—Å–µ active_symbols.
        - –≠—Ç–æ —É–±–∏—Ä–∞–µ—Ç —Å–∫—Ä—ã—Ç–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ ¬´—Ç–æ–ª—å–∫–æ —Ç–æ–ø-10¬ª: —Ç–µ–ø–µ—Ä—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è FULL DATA –ø–æ –≤—Å–µ–º –≤–∫–ª—é—á—ë–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º.
        """
        logger.info("üöÄ Starting PostgreSQL batch ingestors...")
        logger.info(f"üåê Binance REST: {self.binance_base_url}")
        logger.info(f"üåê Binance WS:   {self.binance_ws_url}")

        # 1) –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–∂–µ—Å—Ç–æ—Ä: bookTicker + aggTrade –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        channels_main = ['bookTicker', 'aggTrade']
        symbols_main = self.active_symbols if self.active_symbols else SYMBOLS_200
        db_url: str = str(self.database_url)
        main_ingestor = BatchIngestor(
            db_connection_string=db_url,
            symbols=symbols_main,
            channels=channels_main,
            shards_count=self.shards,
            ws_base_url=self.binance_ws_url,
        )
        self.ingestors.append(main_ingestor)
        asyncio.create_task(main_ingestor.start())
        logger.info(f"‚úÖ Main ingestor (bt/tr) started with {len(symbols_main)} symbols")

    # 2) Depth-–∏–Ω–∂–µ—Å—Ç–æ—Ä: diff depth@100ms –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (FULL DATA)
        if self.enable_depth:
            # FULL DATA –ø–æ –≤—Å–µ–º –∞–∫—Ç–∏–≤–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º DEPTH_TOP_SYMBOLS, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å–∫—Ä—ã—Ç—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            depth_symbols = list(self.active_symbols) if self.active_symbols else list(SYMBOLS_200)

            if depth_symbols:
                db_url: str = str(self.database_url)
                # –®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: 1 —à–∞—Ä–¥ –Ω–∞ –∫–∞–∂–¥—ã–µ ~20 —Å–∏–º–≤–æ–ª–æ–≤, –º–∏–Ω–∏–º—É–º 1, –º–∞–∫—Å–∏–º—É–º 5
                shards_for_depth = max(1, min(5, (len(depth_symbols) + 19) // 20))
                depth_ingestor = BatchIngestor(
                    db_connection_string=db_url,
                    symbols=depth_symbols,
                    channels=['depth@100ms'],
                    shards_count=shards_for_depth,
                    ws_base_url=self.binance_ws_url,
                )
                self.ingestors.append(depth_ingestor)
                asyncio.create_task(depth_ingestor.start())
                logger.info(f"üßä Depth ingestor started for {len(depth_symbols)} symbols (FULL DATA, shards={shards_for_depth})")
            else:
                logger.warning("ENABLE_DEPTH=true, –Ω–æ —Å–ø–∏—Å–æ–∫ depth —Å–∏–º–≤–æ–ª–æ–≤ –ø—É—Å—Ç ‚Äî depth –Ω–µ –∑–∞–ø—É—â–µ–Ω")

        # 3) –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ—Ç–¥–µ–ª—å–Ω—ã–π multi-stream –≤–æ—Ä–∫–µ—Ä –¥–ª—è markPrice/forceOrder
        # –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –≤ —ç—Ç–æ–º –≤–æ—Ä–∫–µ—Ä–µ –æ—Ç–∫–ª—é—á–∞–µ–º base-–∫–∞–Ω–∞–ª—ã –∏ depth.
        if self.enable_mark_price or self.enable_force_order:
            try:
                os.environ.setdefault('ENABLE_BOOK_TICKER', 'false')
                os.environ.setdefault('ENABLE_AGG_TRADE', 'false')
                os.environ.setdefault('ENABLE_DEPTH_TOP', 'false')
                os.environ['ENABLE_MARK_PRICE'] = 'true' if self.enable_mark_price else 'false'
                os.environ['ENABLE_FORCE_ORDER'] = 'true' if self.enable_force_order else 'false'
                from collector.ingestion.multi_stream_collector import MultiStreamCollector
                ms = MultiStreamCollector(db_url, batch_size=200)
                self.ingestors.append(ms)  # –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ shutdown
                asyncio.create_task(ms.initialize())
                asyncio.create_task(ms.start())
                logger.info(f"üè∑Ô∏è MultiStream worker started (markPrice={self.enable_mark_price}, forceOrder={self.enable_force_order})")
            except Exception as e:
                logger.error(f"‚ùå Failed to start MultiStream worker for mark/force: {e}")
    
    async def start_health_monitor(self):
        """–ó–∞–ø—É—Å–∫ health monitoring dashboard"""
        logger.info("üìä Starting health monitoring dashboard...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MonitoringSystem, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–Ω–∏–º–∞–µ—Ç aiohttp dashboard —Å /health
        self.monitoring_system = MonitoringSystem(
            db_connection_string=str(self.database_url),
            dashboard_port=self.monitoring_port
        )
        
        # –ó–∞–ø—É—Å–∫ –≤ background task
        asyncio.create_task(self.monitoring_system.start())
        logger.info(f"‚úÖ Monitoring system started on port {self.monitoring_port}")

        # –ó–∞–ø—É—Å–∫–∞–µ–º watchdog –¥–ª—è –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ë–î
        if self.enable_db_watchdog:
            asyncio.create_task(self._db_watchdog())
            logger.info(
                f"üõ°Ô∏è DB watchdog enabled: interval={self.db_watchdog_interval}s, threshold={self.db_watchdog_threshold}s"
            )

    async def _db_watchdog(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç pg_stat_activity –∏ –æ—Ç–º–µ–Ω—è–µ—Ç –≤–∏—Å—è—á–∏–µ –∑–∞–ø—Ä–æ—Å—ã > threshold."""
        while True:
            try:
                import asyncpg, ssl
                from urllib.parse import urlparse
                # –ù–∞—Å—Ç—Ä–æ–∏–º ssl –∫–∞–∫ –≤ init_database
                ssl_ctx = None
                try:
                    parsed = urlparse(self.database_url)
                    query = {}
                    qstr = str(getattr(parsed, 'query', '') or '')
                    if qstr:
                        for part in qstr.split("&"):
                            if not part:
                                continue
                            if "=" in part:
                                k, v = part.split("=", 1)
                            else:
                                k, v = part, ''
                            query[k] = v
                    sslmode = (query.get('sslmode') or 'require').lower()
                    if sslmode in ('disable', 'allow', 'prefer'):
                        ssl_ctx = False
                    elif sslmode in ('require','verify-none'):
                        ctx = ssl.create_default_context()
                        ctx.check_hostname = False
                        ctx.verify_mode = ssl.CERT_NONE
                        ssl_ctx = ctx
                    elif sslmode in ('verify-full','verify-ca'):
                        ctx = ssl.create_default_context()
                        ctx.check_hostname = True
                        ctx.verify_mode = ssl.CERT_REQUIRED
                        ssl_ctx = ctx
                except Exception:
                    ssl_ctx = None

                conn = await asyncpg.connect(self.database_url, ssl=ssl_ctx)
                try:
                    # –ù–∞—Ö–æ–¥–∏–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –≤–∏—Å—è—â–∏–µ –¥–æ–ª—å—à–µ threshold, –∏—Å–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º–Ω—ã–µ/–Ω–∞—à –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
                    rows = await conn.fetch(
                        """
                        SELECT pid, now() - query_start AS duration, state, application_name, query
                        FROM pg_stat_activity
                        WHERE datname = current_database()
                          AND state = 'active'
                          AND now() - query_start > $1::interval
                          AND application_name NOT IN ('collector_monitor')
                          AND query NOT ILIKE '%pg_stat_activity%'
                        ORDER BY duration DESC
                        LIMIT 20
                        """,
                        timedelta(seconds=self.db_watchdog_threshold),
                    )
                    for r in rows:
                        pid = r['pid']
                        dur = r['duration']
                        app = r['application_name']
                        logger.warning(f"‚ö†Ô∏è Cancelling long-running query pid={pid}, app='{app}', duration={dur}")
                        try:
                            await conn.execute("SELECT pg_cancel_backend($1)", pid)
                        except Exception as ce:
                            logger.error(f"‚ùå Failed to cancel pid={pid}: {ce}")
                finally:
                    await conn.close()
            except Exception as e:
                logger.error(f"DB watchdog error: {e}")
            await asyncio.sleep(self.db_watchdog_interval)
    
    async def wait_for_shutdown(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ"""
        def signal_handler(signum, frame):
            logger.info(f"üì° Received signal {signum}, initiating shutdown...")
            self.shutdown_event.set()
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        await self.shutdown_event.wait()
    
    async def cleanup(self):
        """Graceful shutdown –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        logger.info("üîÑ Starting graceful shutdown...")
        
        tasks = []
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –∏–Ω–∂–µ—Å—Ç–æ—Ä—ã
        for ing in self.ingestors:
            try:
                tasks.append(ing.stop())
            except Exception:
                pass
        
        if self.monitoring_system:
            tasks.append(self.monitoring_system.stop())
        
        if self.db_connection:
            tasks.append(self.db_connection.close())
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info("‚úÖ Cleanup completed")
    
    async def run(self):
        """–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        try:
            logger.info("üöÄ Starting PostgreSQL OrderBook Collector (Production)")
            logger.info(f"üéØ Market Maker Analysis Focus: {len(SYMBOLS_200)} symbols")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            await self.validate_symbols_config()
            await self.init_database()
            
            # –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            await self.start_batch_ingestores()
            await self.start_health_monitor()
            
            logger.info("üéâ All components started successfully!")
            logger.info(f"üìä Monitoring: http://localhost:{self.monitoring_port}/health")
            logger.info("üîÑ Press Ctrl+C to stop")
            
            # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            await self.wait_for_shutdown()
            
        except Exception as e:
            logger.error(f"üí• Fatal error: {e}")
            sys.exit(1)
        finally:
            await self.cleanup()

async def main():
    """Entrypoint –¥–ª—è Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    os.makedirs('/app/logs', exist_ok=True)
    os.makedirs('/app/data', exist_ok=True)
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PostgreSQL
    logger.info("‚è≥ Waiting for PostgreSQL to be ready...")
    for attempt in range(30):
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ë–î
            import asyncpg, ssl
            from urllib.parse import urlparse
            database_url = os.getenv('DATABASE_URL')
            # Honor sslmode=require for DO Postgres
            ssl_ctx = None
            try:
                parsed = urlparse(database_url or '')
                query = {}
                if parsed and parsed.query:
                    for part in parsed.query.split('&'):
                        if not part:
                            continue
                        k, _, v = part.partition('=')
                        query[k] = v
                sslmode = (query.get('sslmode') or 'require').lower()
                if sslmode in ('disable', 'allow', 'prefer'):
                    ssl_ctx = False
                elif sslmode in ('require', 'verify-none'):
                    ctx = ssl.create_default_context()
                    ctx.check_hostname = False
                    ctx.verify_mode = ssl.CERT_NONE
                    ssl_ctx = ctx
                elif sslmode in ('verify-full', 'verify-ca'):
                    ctx = ssl.create_default_context()
                    ctx.check_hostname = True
                    ctx.verify_mode = ssl.CERT_REQUIRED
                    ssl_ctx = ctx
            except Exception:
                ssl_ctx = None
            conn = await asyncpg.connect(database_url, ssl=ssl_ctx)
            await conn.close()
            logger.info("‚úÖ PostgreSQL is ready!")
            break
        except Exception as e:
            if attempt == 29:
                logger.error(f"‚ùå PostgreSQL not available after 30 attempts: {e}")
                sys.exit(1)
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—á–∞—Ç–∞–µ–º –ø—Ä–∏—á–∏–Ω—É, —á—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, firewall/SSL)
            if attempt == 0 or (attempt + 1) % 5 == 0:
                logger.warning(f"‚è≥ Attempt {attempt + 1}/30: PostgreSQL not ready: {e}")
            else:
                logger.info(f"‚è≥ Attempt {attempt + 1}/30: PostgreSQL not ready, waiting...")
            time.sleep(2)
    
    # –ó–∞–ø—É—Å–∫ collector
    collector = ProductionCollector()
    await collector.run()

if __name__ == "__main__":
    asyncio.run(main())