#!/usr/bin/env python3
"""
Docker Entrypoint –¥–ª—è PostgreSQL OrderBook Collector
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å 200 —Å–∏–º–≤–æ–ª–∞–º–∏ MM —Ñ–æ–∫—É—Å–∞
"""

import asyncio
import os
import sys
import logging
import signal
import time
from pathlib import Path
import aiohttp
from typing import cast

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ PYTHONPATH
sys.path.insert(0, '/app')

from collector.config.symbols_mm_focused import SYMBOLS_200, validate_symbols
from collector.ingestion.batch_ingestor import BatchIngestor
from collector.monitoring.health_monitor import MonitoringSystem
from collector.database.connection import DatabaseConnection

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/app/logs/collector.log')
    ]
)
logger = logging.getLogger(__name__)

class ProductionCollector:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è production —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è"""
    
    def __init__(self):
        # –ù–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–∂–µ—Å—Ç–æ—Ä–æ–≤: –æ—Å–Ω–æ–≤–Ω–æ–π (bt/tr) –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –¥–ª—è depth
        self.ingestors = []
        self.monitoring_system = None
        self.db_connection = None
        self.shutdown_event = asyncio.Event()
        self.active_symbols = []
        
        # Environment variables
        self.database_url = os.getenv('DATABASE_URL')
        self.batch_size = int(os.getenv('BATCH_SIZE', '500'))
        self.flush_interval = int(os.getenv('FLUSH_INTERVAL', '30'))
        self.shards = int(os.getenv('SHARDS', '5'))
        self.monitoring_port = int(os.getenv('MONITORING_PORT', '8000'))
        self.binance_base_url = os.getenv('BINANCE_BASE_URL', 'https://fapi.binance.com').strip()
        self.binance_ws_url = os.getenv('BINANCE_WS_URL', 'wss://fstream.binance.com/ws/').strip()
        # Depth –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.enable_depth = os.getenv('ENABLE_DEPTH', 'false').strip().lower() in ('1', 'true', 'yes')
        self.depth_top_symbols_env = os.getenv('DEPTH_TOP_SYMBOLS', '')
        
        if not self.database_url:
            raise ValueError("DATABASE_URL environment variable is required")
        # –î–ª—è —Ç–∞–π–ø—á–µ–∫–µ—Ä–∞ –∏ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ str
        self.database_url = cast(str, self.database_url)
    
    async def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã"""
        logger.info("üîß Initializing database connection...")
        db_url: str = str(self.database_url)
        self.db_connection = DatabaseConnection(db_url)
        await self.db_connection.connect()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        schema_file = Path('/app/collector/database/schema.sql')
        if schema_file.exists():
            logger.info("üìã Creating database schema...")
            try:
                with open(schema_file, 'r') as f:
                    schema_sql = f.read()
                # –ü–æ–Ω–∏–∂–∞–µ–º lock_timeout –Ω–∞ —Å–µ—Å—Å–∏—é, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö –æ–±—ä–µ–∫—Ç–∞—Ö
                try:
                    await self.db_connection.execute_script("SET lock_timeout TO '5s';")
                except Exception:
                    pass
                await self.db_connection.execute_script(schema_sql)
                logger.info("‚úÖ Database schema created successfully")
            except Exception as e:
                # –ù–µ —Ñ–µ–π–ª–∏–º –≤–µ—Å—å –∑–∞–ø—É—Å–∫: —Å—Ö–µ–º—ã —É–∂–µ –µ—Å—Ç—å, –æ—à–∏–±–æ–∫ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞
                logger.warning(f"‚ö†Ô∏è Schema creation skipped due to error: {e}")

            # –î–æ–ø. –≥–∞—Ä–∞–Ω—Ç–∏—è: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ON CONFLICT –Ω–∞ depth_events
            # –í –ø—Ä–æ–¥–µ –º–æ–≥ –±—ã—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç —Ä–∞–Ω–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ PK/unique ‚Äî —Å–æ–∑–¥–∞—ë–º idempotent-–∏–Ω–¥–µ–∫—Å
            try:
                ensure_idx_sql = (
                    """
                    CREATE UNIQUE INDEX IF NOT EXISTS uq_depth_events_symbol_time_final
                    ON marketdata.depth_events (symbol_id, ts_exchange, final_update_id);
                    """
                )
                await self.db_connection.execute_script(ensure_idx_sql)
                logger.info("‚úÖ Ensured unique index on marketdata.depth_events (symbol_id, ts_exchange, final_update_id)")
            except Exception as e:
                logger.error(f"‚ùå Failed to ensure unique index for depth_events: {e}")
        else:
            logger.warning("‚ö†Ô∏è Schema file not found, skipping schema creation")
    
    async def validate_symbols_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        logger.info("üéØ Validating Market Maker symbols configuration...")
        
        try:
            validate_symbols()
            logger.info(f"‚úÖ Validated {len(SYMBOLS_200)} symbols for MM analysis")
            logger.info(f"üìä Starting with: {SYMBOLS_200[0]}")
            logger.info(f"üìä Ultra low-cap symbols: {len(SYMBOLS_200[-30:])}")

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º Binance Futures USDT-–ø–µ—Ä–ø–∞–º
            self.active_symbols = await self._resolve_futures_symbols(SYMBOLS_200)
            logger.info(f"‚úÖ Resolved {len(self.active_symbols)} valid Futures symbols out of {len(SYMBOLS_200)}")
            if len(self.active_symbols) < len(SYMBOLS_200):
                missing = len(SYMBOLS_200) - len(self.active_symbols)
                logger.warning(f"‚ö†Ô∏è Filtered out {missing} symbols not present on Binance Futures USDT-perp")
        except Exception as e:
            logger.error(f"‚ùå Symbol validation failed: {e}")
            raise

    async def _resolve_futures_symbols(self, candidates):
        """–ó–∞–ø—Ä–æ—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö USDT-–ø–µ—Ä–ø–µ—Ç—É–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ Binance Futures –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."""
        base = self.binance_base_url.rstrip('/')
        url = f"{base}/fapi/v1/exchangeInfo"
        timeout = aiohttp.ClientTimeout(total=10)
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    symbols = data.get('symbols', [])
                    allowed = set(
                        s.get('symbol') for s in symbols
                        if s.get('contractType') in ('PERPETUAL', 'CURRENT_QUARTER', 'NEXT_QUARTER')
                        and s.get('status') == 'TRADING'
                        and s.get('quoteAsset') == 'USDT'
                    )
                    filtered = [sym for sym in candidates if sym in allowed]
                    return filtered
        except Exception as e:
            logger.error(f"‚ùå Failed to resolve futures symbols from {url}: {e}. Fallback to original list.")
            return list(candidates)
    
    async def start_batch_ingestores(self):
        """–ó–∞–ø—É—Å–∫ batch –∏–Ω–∂–µ—Å—Ç–æ—Ä–æ–≤: –æ—Å–Ω–æ–≤–Ω–æ–π (bt/tr) + –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ depth@100ms –¥–ª—è —Ç–æ–ø-—Å–∏–º–≤–æ–ª–æ–≤"""
        logger.info("üöÄ Starting PostgreSQL batch ingestors...")
        logger.info(f"üåê Binance REST: {self.binance_base_url}")
        logger.info(f"üåê Binance WS:   {self.binance_ws_url}")

        # 1) –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–∂–µ—Å—Ç–æ—Ä: bookTicker + aggTrade –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        channels_main = ['bookTicker', 'aggTrade']
        symbols_main = self.active_symbols if self.active_symbols else SYMBOLS_200
        db_url: str = str(self.database_url)
        main_ingestor = BatchIngestor(
            db_connection_string=db_url,
            symbols=symbols_main,
            channels=channels_main,
            shards_count=self.shards,
            ws_base_url=self.binance_ws_url,
        )
        self.ingestors.append(main_ingestor)
        asyncio.create_task(main_ingestor.start())
        logger.info(f"‚úÖ Main ingestor (bt/tr) started with {len(symbols_main)} symbols")

    # 2) –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π depth-–∏–Ω–∂–µ—Å—Ç–æ—Ä: diff depth@100ms —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Å–∏–º–≤–æ–ª–æ–≤
        if self.enable_depth:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: DEPTH_TOP_SYMBOLS –∏–∑ ENV; –∏–Ω–∞—á–µ –≤–æ–∑—å–º–µ–º –ø–µ—Ä–≤—ã–µ 10 –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            depth_symbols = []
            if self.depth_top_symbols_env.strip():
                depth_symbols = [s.strip().upper() for s in self.depth_top_symbols_env.split(',') if s.strip()]
                # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ futures —Å–∏–º–≤–æ–ª—ã
                valid_set = set(self.active_symbols) if self.active_symbols else set(SYMBOLS_200)
                depth_symbols = [s for s in depth_symbols if s in valid_set]
            if not depth_symbols:
                src = self.active_symbols if self.active_symbols else SYMBOLS_200
                depth_symbols = src[:10]

            if depth_symbols:
                db_url: str = str(self.database_url)
                depth_ingestor = BatchIngestor(
                    db_connection_string=db_url,
                    symbols=depth_symbols,
                    channels=['depth@100ms'],
                    shards_count=max(1, min(2, len(depth_symbols)//5)),  # 1-2 —à–∞—Ä–¥–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
                    ws_base_url=self.binance_ws_url,
                )
                self.ingestors.append(depth_ingestor)
                asyncio.create_task(depth_ingestor.start())
                logger.info(f"üßä Depth ingestor started for {len(depth_symbols)} symbols: {depth_symbols}")
            else:
                logger.warning("ENABLE_DEPTH=true, –Ω–æ —Å–ø–∏—Å–æ–∫ depth —Å–∏–º–≤–æ–ª–æ–≤ –ø—É—Å—Ç ‚Äî depth –Ω–µ –∑–∞–ø—É—â–µ–Ω")
    
    async def start_health_monitor(self):
        """–ó–∞–ø—É—Å–∫ health monitoring dashboard"""
        logger.info("üìä Starting health monitoring dashboard...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MonitoringSystem, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–Ω–∏–º–∞–µ—Ç aiohttp dashboard —Å /health
        self.monitoring_system = MonitoringSystem(
            db_connection_string=str(self.database_url),
            dashboard_port=self.monitoring_port
        )
        
        # –ó–∞–ø—É—Å–∫ –≤ background task
        asyncio.create_task(self.monitoring_system.start())
        logger.info(f"‚úÖ Monitoring system started on port {self.monitoring_port}")
    
    async def wait_for_shutdown(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ"""
        def signal_handler(signum, frame):
            logger.info(f"üì° Received signal {signum}, initiating shutdown...")
            self.shutdown_event.set()
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        await self.shutdown_event.wait()
    
    async def cleanup(self):
        """Graceful shutdown –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        logger.info("üîÑ Starting graceful shutdown...")
        
        tasks = []
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –∏–Ω–∂–µ—Å—Ç–æ—Ä—ã
        for ing in self.ingestors:
            try:
                tasks.append(ing.stop())
            except Exception:
                pass
        
        if self.monitoring_system:
            tasks.append(self.monitoring_system.stop())
        
        if self.db_connection:
            tasks.append(self.db_connection.close())
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info("‚úÖ Cleanup completed")
    
    async def run(self):
        """–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        try:
            logger.info("üöÄ Starting PostgreSQL OrderBook Collector (Production)")
            logger.info(f"üéØ Market Maker Analysis Focus: {len(SYMBOLS_200)} symbols")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            await self.validate_symbols_config()
            await self.init_database()
            
            # –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            await self.start_batch_ingestores()
            await self.start_health_monitor()
            
            logger.info("üéâ All components started successfully!")
            logger.info(f"üìä Monitoring: http://localhost:{self.monitoring_port}/health")
            logger.info("üîÑ Press Ctrl+C to stop")
            
            # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            await self.wait_for_shutdown()
            
        except Exception as e:
            logger.error(f"üí• Fatal error: {e}")
            sys.exit(1)
        finally:
            await self.cleanup()

async def main():
    """Entrypoint –¥–ª—è Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    os.makedirs('/app/logs', exist_ok=True)
    os.makedirs('/app/data', exist_ok=True)
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PostgreSQL
    logger.info("‚è≥ Waiting for PostgreSQL to be ready...")
    for attempt in range(30):
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ë–î
            import asyncpg
            database_url = os.getenv('DATABASE_URL')
            conn = await asyncpg.connect(database_url)
            await conn.close()
            logger.info("‚úÖ PostgreSQL is ready!")
            break
        except Exception as e:
            if attempt == 29:
                logger.error(f"‚ùå PostgreSQL not available after 30 attempts: {e}")
                sys.exit(1)
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—á–∞—Ç–∞–µ–º –ø—Ä–∏—á–∏–Ω—É, —á—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, firewall/SSL)
            if attempt == 0 or (attempt + 1) % 5 == 0:
                logger.warning(f"‚è≥ Attempt {attempt + 1}/30: PostgreSQL not ready: {e}")
            else:
                logger.info(f"‚è≥ Attempt {attempt + 1}/30: PostgreSQL not ready, waiting...")
            time.sleep(2)
    
    # –ó–∞–ø—É—Å–∫ collector
    collector = ProductionCollector()
    await collector.run()

if __name__ == "__main__":
    asyncio.run(main())