"""
üöÄ ENHANCED MULTI-STREAM COLLECTOR V2.0
=======================================

–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞ —Å:
- Advanced error handling & exponential backoff
- 200+ —Å–∏–º–≤–æ–ª–æ–≤ —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º 
- Circuit breaker patterns
- Comprehensive monitoring & health checks
- Automatic symbol loading –≤ PostgreSQL
"""

import asyncio
import logging
import signal
import json
import time
import random
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum

import asyncpg
import websockets
from websockets.exceptions import ConnectionClosed, WebSocketException

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.symbols_config import (
    get_symbol_shards, ALL_SYMBOLS, RATE_LIMITS
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/enhanced_collector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ConnectionState(Enum):
    DISCONNECTED = "disconnected"
    CONNECTING = "connecting" 
    CONNECTED = "connected"
    RECONNECTING = "reconnecting"
    FAILED = "failed"

@dataclass
class StreamMetrics:
    symbols_count: int = 0
    messages_received: int = 0
    messages_processed: int = 0
    messages_failed: int = 0
    last_message_time: Optional[float] = None
    connection_state: ConnectionState = ConnectionState.DISCONNECTED
    reconnect_count: int = 0
    last_error: Optional[str] = None

@dataclass
class BatchBuffer:
    table_name: str
    data: List[tuple]
    max_size: int
    last_flush: float
    flush_interval: float
    
    def should_flush(self) -> bool:
        return (
            len(self.data) >= self.max_size or
            time.time() - self.last_flush >= self.flush_interval
        )
    
    def add(self, record: tuple):
        self.data.append(record)
    
    def clear(self):
        self.data.clear()
        self.last_flush = time.time()

class CircuitBreaker:
    """Circuit breaker –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–∞—Å–∫–∞–¥–Ω—ã—Ö –æ—Ç–∫–∞–∑–æ–≤"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func):
        """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±—ë—Ä—Ç–∫–∏ —Ñ—É–Ω–∫—Ü–∏–π –≤ circuit breaker"""
        async def wrapper(*args, **kwargs):
            if self.state == 'OPEN':
                if self.last_failure_time and time.time() - self.last_failure_time < self.recovery_timeout:
                    raise Exception("Circuit breaker is OPEN")
                else:
                    self.state = 'HALF_OPEN'
            
            try:
                result = await func(*args, **kwargs)
                if self.state == 'HALF_OPEN':
                    self.state = 'CLOSED'
                    self.failure_count = 0
                return result
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.failure_threshold:
                    self.state = 'OPEN'
                    logger.error(f"Circuit breaker opened after {self.failure_count} failures")
                
                raise e
        
        return wrapper

class EnhancedWebSocketStream:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π WebSocket –ø–æ—Ç–æ–∫ —Å advanced error handling"""
    
    def __init__(self, stream_id: str, symbols: List[str], stream_type: str, 
                 batch_processor, priority: str = 'medium'):
        self.stream_id = stream_id
        self.symbols = symbols
        self.stream_type = stream_type
        self.batch_processor = batch_processor
        self.priority = priority
        
        self.metrics = StreamMetrics(symbols_count=len(symbols))
        self.circuit_breaker = CircuitBreaker()
        
        self.websocket = None
        self.reconnect_delays = RATE_LIMITS['reconnect_delay']
        self.current_delay_index = 0
        self.max_reconnect_attempts = 10
        
        self.should_stop = False
        
    def _build_stream_url(self) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        base_url = "wss://fstream.binance.com/ws/"
        
        if self.stream_type == 'bookTicker':
            streams = [f"{symbol.lower()}@bookTicker" for symbol in self.symbols]
        elif self.stream_type == 'aggTrade':
            streams = [f"{symbol.lower()}@aggTrade" for symbol in self.symbols]
        elif self.stream_type.startswith('depth'):
            streams = [f"{symbol.lower()}@{self.stream_type}" for symbol in self.symbols]
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–æ—Ç–æ–∫–∞: {self.stream_type}")
        
        return base_url + "/".join(streams)
    
    async def _connect(self) -> bool:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ WebSocket —Å retry logic"""
        self.metrics.connection_state = ConnectionState.CONNECTING
        
        try:
            url = self._build_stream_url()
            logger.info(f"üîó [{self.stream_id}] –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ {len(self.symbols)} —Å–∏–º–≤–æ–ª–æ–≤ ({self.stream_type})")
            
            self.websocket = await websockets.connect(
                url,
                ping_interval=20,
                ping_timeout=10,
                close_timeout=10
            )
            
            self.metrics.connection_state = ConnectionState.CONNECTED
            self.current_delay_index = 0  # –°–±—Ä–æ—Å –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏
            logger.info(f"‚úÖ [{self.stream_id}] –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ WebSocket")
            return True
            
        except Exception as e:
            self.metrics.connection_state = ConnectionState.FAILED
            self.metrics.last_error = str(e)
            logger.error(f"‚ùå [{self.stream_id}] –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False
    
    async def _handle_message(self, message: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å circuit breaker"""
        try:
            data = json.loads(message)
            self.metrics.messages_received += 1
            self.metrics.last_message_time = time.time()
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ circuit breaker
            await self.circuit_breaker.call(self._process_message)(data)
            self.metrics.messages_processed += 1
            
        except Exception as e:
            self.metrics.messages_failed += 1
            self.metrics.last_error = str(e)
            logger.error(f"‚ùå [{self.stream_id}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
    
    async def _process_message(self, data: dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if 'stream' in data:
            # Multi-stream —Ñ–æ—Ä–º–∞—Ç
            stream_name = data['stream']
            event_data = data['data']
        else:
            # Single stream —Ñ–æ—Ä–º–∞—Ç
            event_data = data
            stream_name = self.stream_type
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è –∏ –ø–µ—Ä–µ–¥–∞—ë–º –≤ batch processor
        if '@bookTicker' in stream_name:
            await self.batch_processor.add_book_ticker_event(event_data)
        elif '@aggTrade' in stream_name:
            await self.batch_processor.add_trade_event(event_data)
        elif '@depth' in stream_name:
            await self.batch_processor.add_depth_event(event_data)
    
    async def _reconnect_with_backoff(self) -> bool:
        """–ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å exponential backoff"""
        if self.current_delay_index >= len(self.reconnect_delays):
            logger.error(f"‚ùå [{self.stream_id}] –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
            return False
        
        delay = self.reconnect_delays[self.current_delay_index]
        self.current_delay_index += 1
        
        logger.info(f"üîÑ [{self.stream_id}] –ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ {delay} —Å–µ–∫—É–Ω–¥...")
        await asyncio.sleep(delay)
        
        self.metrics.reconnect_count += 1
        self.metrics.connection_state = ConnectionState.RECONNECTING
        
        return await self._connect()
    
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –ø–æ—Ç–æ–∫–∞"""
        logger.info(f"üöÄ [{self.stream_id}] –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞: {len(self.symbols)} —Å–∏–º–≤–æ–ª–æ–≤, {self.stream_type}")
        
        while not self.should_stop:
            try:
                # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
                if not await self._connect():
                    if not await self._reconnect_with_backoff():
                        break
                    continue
                
                # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
                if self.websocket:
                    async for message in self.websocket:
                        if self.should_stop:
                            break
                        
                        await self._handle_message(message)
                    
            except ConnectionClosed:
                logger.warning(f"‚ö†Ô∏è [{self.stream_id}] WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ")
                if not self.should_stop and not await self._reconnect_with_backoff():
                    break
                    
            except WebSocketException as e:
                logger.error(f"‚ùå [{self.stream_id}] WebSocket –æ—à–∏–±–∫–∞: {e}")
                if not await self._reconnect_with_backoff():
                    break
                    
            except Exception as e:
                logger.error(f"‚ùå [{self.stream_id}] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                if not await self._reconnect_with_backoff():
                    break
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        if self.websocket:
            await self.websocket.close()
        
        self.metrics.connection_state = ConnectionState.DISCONNECTED
        logger.info(f"üõë [{self.stream_id}] –ü–æ—Ç–æ–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞"""
        self.should_stop = True

class EnhancedBatchProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π batch processor —Å adaptive batching"""
    
    def __init__(self, pg_pool):
        self.pg_pool = pg_pool
        self.symbol_id_cache = {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±—É—Ñ–µ—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.buffers = {
            'book_ticker': BatchBuffer('book_ticker', [], 1000, time.time(), 5.0),
            'trades': BatchBuffer('trades', [], 500, time.time(), 3.0),
            'depth_events': BatchBuffer('depth_events', [], 100, time.time(), 2.0)
        }
        
        self.stats = {
            'book_ticker': {'success': 0, 'failed': 0},
            'trades': {'success': 0, 'failed': 0},
            'depth_events': {'success': 0, 'failed': 0}
        }
        
        # –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ flush
        self.flush_task = None
        
    async def start(self):
        """–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ flush"""
        await self._load_symbol_cache()
        self.flush_task = asyncio.create_task(self._periodic_flush())
        
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π flush"""
        if self.flush_task:
            self.flush_task.cancel()
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π flush –≤—Å–µ—Ö –±—É—Ñ–µ—Ä–æ–≤
        for buffer_name in self.buffers:
            await self._flush_buffer(buffer_name)
    
    async def _load_symbol_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ symbol_id –∏–∑ PostgreSQL"""
        async with self.pg_pool.acquire() as conn:
            symbols = await conn.fetch('SELECT id, symbol FROM marketdata.symbols')
            self.symbol_id_cache = {row['symbol']: row['id'] for row in symbols}
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.symbol_id_cache)} —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∫—ç—à")
    
    async def _get_symbol_id(self, symbol: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ symbol_id —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if symbol in self.symbol_id_cache:
            return self.symbol_id_cache[symbol]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Å–∏–º–≤–æ–ª
        async with self.pg_pool.acquire() as conn:
            symbol_id = await conn.fetchval("""
                INSERT INTO marketdata.symbols (exchange, symbol, is_active)
                VALUES ('binance-futures', $1, true)
                ON CONFLICT (exchange, symbol) DO UPDATE SET updated_at = now()
                RETURNING id
            """, symbol)
            
            self.symbol_id_cache[symbol] = symbol_id
            logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π —Å–∏–º–≤–æ–ª: {symbol} (ID: {symbol_id})")
            return symbol_id
    
    async def add_book_ticker_event(self, data: dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ book ticker —Å–æ–±—ã—Ç–∏—è –≤ –±—É—Ñ–µ—Ä"""
        try:
            symbol = data['s']
            symbol_id = await self._get_symbol_id(symbol)
            
            ts_exchange = datetime.fromtimestamp(data['E'] / 1000, tz=timezone.utc)
            ts_ingest = datetime.now(timezone.utc)
            
            # –†–∞—Å—á—ë—Ç derived –ø–æ–ª–µ–π
            best_bid = float(data['b'])
            best_ask = float(data['a'])
            bid_qty = float(data['B'])
            ask_qty = float(data['A'])
            spread = best_ask - best_bid
            mid = (best_ask + best_bid) / 2
            
            record = (
                ts_exchange, ts_ingest, symbol_id, None,  # update_id –∏–∑ stream –º–æ–∂–µ—Ç –±—ã—Ç—å None
                best_bid, best_ask, bid_qty, ask_qty, spread, mid
            )
            
            self.buffers['book_ticker'].add(record)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å flush
            if self.buffers['book_ticker'].should_flush():
                await self._flush_buffer('book_ticker')
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è book_ticker —Å–æ–±—ã—Ç–∏—è: {e}")
    
    async def add_trade_event(self, data: dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ trade —Å–æ–±—ã—Ç–∏—è –≤ –±—É—Ñ–µ—Ä"""
        try:
            symbol = data['s']
            symbol_id = await self._get_symbol_id(symbol)
            
            ts_exchange = datetime.fromtimestamp(data['T'] / 1000, tz=timezone.utc)
            ts_ingest = datetime.now(timezone.utc)
            
            record = (
                ts_exchange, ts_ingest, symbol_id, int(data['a']),
                float(data['p']), float(data['q']), data['m']
            )
            
            self.buffers['trades'].add(record)
            
            if self.buffers['trades'].should_flush():
                await self._flush_buffer('trades')
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è trade —Å–æ–±—ã—Ç–∏—è: {e}")
    
    async def add_depth_event(self, data: dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ depth —Å–æ–±—ã—Ç–∏—è –≤ –±—É—Ñ–µ—Ä"""
        try:
            symbol = data['s']
            symbol_id = await self._get_symbol_id(symbol)
            
            ts_exchange = datetime.fromtimestamp(data['E'] / 1000, tz=timezone.utc)
            ts_ingest = datetime.now(timezone.utc)
            
            record = (
                ts_exchange, ts_ingest, symbol_id,
                int(data['U']), int(data['u']), int(data.get('pu', 0)),
                json.dumps(data['b']), json.dumps(data['a'])
            )
            
            self.buffers['depth_events'].add(record)
            
            if self.buffers['depth_events'].should_flush():
                await self._flush_buffer('depth_events')
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è depth —Å–æ–±—ã—Ç–∏—è: {e}")
    
    async def _flush_buffer(self, table_name: str):
        """Flush –±—É—Ñ–µ—Ä–∞ –≤ PostgreSQL"""
        buffer = self.buffers[table_name]
        if not buffer.data:
            return
        
        try:
            async with self.pg_pool.acquire() as conn:
                if table_name == 'book_ticker':
                    await conn.executemany("""
                        INSERT INTO marketdata.book_ticker (
                            ts_exchange, ts_ingest, symbol_id, update_id,
                            best_bid, best_ask, bid_qty, ask_qty, spread, mid
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    """, buffer.data)
                    
                elif table_name == 'trades':
                    await conn.executemany("""
                        INSERT INTO marketdata.trades (
                            ts_exchange, ts_ingest, symbol_id, agg_trade_id,
                            price, qty, is_buyer_maker
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                        ON CONFLICT (symbol_id, ts_exchange, agg_trade_id) DO NOTHING
                    """, buffer.data)
                    
                elif table_name == 'depth_events':
                    await conn.executemany("""
                        INSERT INTO marketdata.depth_events (
                            ts_exchange, ts_ingest, symbol_id,
                            first_update_id, final_update_id, prev_final_update_id,
                            bids, asks
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (symbol_id, ts_exchange, final_update_id) DO NOTHING
                    """, buffer.data)
                
                self.stats[table_name]['success'] += len(buffer.data)
                logger.info(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ {len(buffer.data)} –∑–∞–ø–∏—Å–µ–π –≤ {table_name}")
                
        except Exception as e:
            self.stats[table_name]['failed'] += len(buffer.data)
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ flush {table_name}: {e}")
        
        buffer.clear()
    
    async def _periodic_flush(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π flush –±—É—Ñ–µ—Ä–æ–≤"""
        while True:
            try:
                await asyncio.sleep(1)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
                
                for table_name, buffer in self.buffers.items():
                    if buffer.should_flush():
                        await self._flush_buffer(table_name)
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ periodic flush: {e}")

class EnhancedMultiStreamCollector:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ multi-stream –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.pg_pool = None
        self.batch_processor = None
        self.streams = []
        self.should_stop = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.start_time = time.time()
        self.total_symbols = 0
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞"""
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Enhanced Multi-Stream Collector v2.0")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ connection pool
        self.pg_pool = await asyncpg.create_pool(
            self.connection_string,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
        logger.info("‚úÖ PostgreSQL pool —Å–æ–∑–¥–∞–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è batch processor
        self.batch_processor = EnhancedBatchProcessor(self.pg_pool)
        await self.batch_processor.start()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ PostgreSQL
        await self._ensure_all_symbols_loaded()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ WebSocket –ø–æ—Ç–æ–∫–æ–≤
        await self._create_streams()
        
    async def _ensure_all_symbols_loaded(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ PostgreSQL"""
        async with self.pg_pool.acquire() as conn:
            for symbol in ALL_SYMBOLS:
                await conn.execute("""
                    INSERT INTO marketdata.symbols (exchange, symbol, is_active)
                    VALUES ('binance-futures', $1, true)
                    ON CONFLICT (exchange, symbol) DO NOTHING
                """, symbol)
        
        count = await self.pg_pool.fetchval('SELECT COUNT(*) FROM marketdata.symbols WHERE is_active = true')
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –ë–î: {count}")
        
    async def _create_streams(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ WebSocket –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        shards = get_symbol_shards()
        self.total_symbols = len(ALL_SYMBOLS)
        
        logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ {len(shards)} WebSocket –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è {self.total_symbols} —Å–∏–º–≤–æ–ª–æ–≤")
        
        for i, shard in enumerate(shards):
            stream_id = f"stream_{i+1}_{shard['priority']}_{shard['stream_type']}"
            
            stream = EnhancedWebSocketStream(
                stream_id=stream_id,
                symbols=shard['symbols'],
                stream_type=shard['stream_type'],
                batch_processor=self.batch_processor,
                priority=shard['priority']
            )
            
            self.streams.append(stream)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.streams)} –ø–æ—Ç–æ–∫–æ–≤")
        
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞"""
        logger.info("‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫ Enhanced Multi-Stream Collector")
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤
        tasks = []
        for stream in self.streams:
            task = asyncio.create_task(stream.run())
            tasks.append(task)
            logger.info(f"üî¥ {stream.stream_id} –∑–∞–ø—É—â–µ–Ω")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats_task = asyncio.create_task(self._stats_monitor())
        tasks.append(stats_task)
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
        
    async def _stats_monitor(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        while not self.should_stop:
            await asyncio.sleep(60)  # –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
            
            try:
                # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫
                total_messages = sum(s.metrics.messages_processed for s in self.streams)
                active_streams = len([s for s in self.streams if s.metrics.connection_state == ConnectionState.CONNECTED])
                
                logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                logger.info(f"   –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: {active_streams}/{len(self.streams)}")
                logger.info(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {total_messages:,}")
                logger.info(f"   –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {int(time.time() - self.start_time)} —Å–µ–∫—É–Ω–¥")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ batch processor
                for table, stats in self.batch_processor.stats.items():
                    logger.info(f"   {table}: {stats['success']:,} ‚úÖ / {stats['failed']:,} ‚ùå")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞"""
        logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Enhanced Multi-Stream Collector")
        self.should_stop = True
        
        for stream in self.streams:
            stream.stop()
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.batch_processor:
            await self.batch_processor.stop()
        
        if self.pg_pool:
            await self.pg_pool.close()
        
        logger.info("‚úÖ –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # PostgreSQL connection string
    connection_string = 'postgresql://user:password@host:port/database'
    
    collector = EnhancedMultiStreamCollector(connection_string)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
    def signal_handler(signum, frame):
        logger.info(f"üì® –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}")
        collector.stop()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        await collector.initialize()
        await collector.start()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        await collector.cleanup()

if __name__ == "__main__":
    asyncio.run(main())